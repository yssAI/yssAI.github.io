
## 使用LSTM生成古诗词
### 循环神经网络和LSTM介绍
序列化数据即每个样本和它之前的样本存在关联，前一数据和后一个数据有顺序关系。深度学习中有一个重要的分支是专门用来处理这样的数据的——循环神经网络。循环神经网络广泛应用在自然语言处理领域，今天我们带你从一个实际的例子出发，介绍神经网络一个重要的改进算法模型，LSTM。本文章不对LSTM的原理进行深入，想详细了解LSTM的可以参考下面的[文章链接](https://www.jianshu.com/p/9dc9f41f0b29)。本文重点从古诗词自动生成的实例出发，一步一步带你从数据处理到模型搭建，再到训练出古诗词生成模型，最后实现从古诗词自动生成新春祝福诗词。

### 数据处理
同时由于我们处理的是文本信息，因此我们需要将每个字都采用词（字）向量的形式表示，由于没有现成的词向量，所有我们要在LSTM的前面假加入一个词嵌入层。
最后，为了避免最终的分类数过于庞大，可以选择去掉出现频率较小的字，比如可以去掉只出现过一次的字。

用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种

总结一下数据预处理的步骤：
- 统计出所有不同的字，并做成一个字典；
- 对于每首诗，将每个字、标点都转换为字典中对应的编号，构成X；
- 将X整体左移动以为构成Y

先来看看原始的数据集长什么样：
![image](https://user-images.githubusercontent.com/43362551/51824023-221ea180-231c-11e9-8577-6595844d752f.png)
数据集共分为以下三个部分：

poem_ids.txt：处理后的语料库文件  
poems_edge_split.txt：原始语料库文件  
rhyme_words.txt：  
vectors_poem.bin：  

代码如下：
``` python

```

### 模型构建

### 模型训练

