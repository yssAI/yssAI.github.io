
## 新年使用LSTM智能作诗送祝福

### LSTM介绍
序列化数据即每个样本和它之前的样本存在关联，前一数据和后一个数据有顺序关系。深度学习中有一个重要的分支是专门用来处理这样的数据的——循环神经网络。循环神经网络广泛应用在自然语言处理领域，今天我们带你从一个实际的例子出发，介绍神经网络一个重要的改进算法模型，LSTM。本文章不对LSTM的原理进行深入，想详细了解LSTM的可以参考下面的[文章链接](https://www.jianshu.com/p/9dc9f41f0b29)。本文重点从古诗词自动生成的实例出发，一步一步带你从数据处理到模型搭建，再到训练出古诗词生成模型，最后实现从古诗词自动生成新春祝福诗词。

### 数据处理

我们使用76748首古诗词作为数据集，数据集下载链接[]，原始的古诗词的存储形式如下：
![image](https://user-images.githubusercontent.com/43362551/51824023-221ea180-231c-11e9-8577-6595844d752f.png)
原始的古诗词是文本的形式，是一种符号的形式，无法直接进行机器学习，所以我们第一步需要把文本信息转换为数据形式，这种转换方式就叫词嵌入(word embedding)，我们采用一种常用的词嵌套(word embedding)算法-Word2vec对古诗词进行编码。关于Word2Vec这里不详细讲解，有兴趣的可以参考下面的[文章链接](https://zhuanlan.zhihu.com/p/26306795)。在词嵌套过程中，为了避免最终的分类数过于庞大，可以选择去掉出现频率较小的字，比如可以去掉只出现过一次的字。Word2vec算法经过训练后会产生一个模型文件，这样我们就可以应用这个Word2Vec模型对古诗词文本进行词嵌套编码。

经过第一步的处理已经把古诗词词语转换为可以机器学习建模的数字形式，因为我们采用循环神经网络进行古诗词生成，所以还需要构建输入到输出的映射处理。例如：
“[长河落日圆]”作为train_data，而相应的train_label就是“长河落日圆]]”，也就是
“[”->“长”，“长”->“河”，“河”->“落”，“落”->“日”，“日”->“圆”，“圆”->“]”，“]”->“]”，这样子先后顺序一一对相。这也是循环神经网络的一个重要的特征。
这里的“[”和“]”是开始符和结束符，用于生成古诗的开始与结束标记。



同时由于我们处理的是文本信息，因此我们需要将每个字都采用词（字）向量的形式表示，由于没有现成的词向量，所有我们要在LSTM的前面假加入一个词嵌入层。
最后，为了避免最终的分类数过于庞大，可以选择去掉出现频率较小的字，比如可以去掉只出现过一次的字。

用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种


总结一下数据预处理的步骤：
- 统计出所有不同的字，并做成一个字典；
- 对于每首诗，将每个字、标点都转换为字典中对应的编号，构成X；
- 将X整体左移动以为构成Y

先来看看原始的数据集长什么样：
![image](https://user-images.githubusercontent.com/43362551/51824023-221ea180-231c-11e9-8577-6595844d752f.png)

如“[大漠孤烟直]”作为train_data，而相应的train_label就是“大漠孤烟直]]”，也就是
“[”->“大”，“大”->“漠”，“漠”->“孤”，“孤”->“烟”，“烟”->“直”，“直”->“]”，“]”->“]”，这样子先后顺序一一对相。这也是RNN的一个重要的特征。
这里的“[”和“]”是开始符和结束符，用于生成唐诗的开始与结束标记。


poem_ids.txt：处理后的语料库文件  
poems_edge_split.txt：原始语料库文件  
rhyme_words.txt： 押韵词存储，用于押韵诗的生成 
vectors_poem.bin：利用Word2Vec训练好的词向量模型，以</s>开头，按词频排列，去除低频词。并且编码。


代码如下：
``` python

```

### 模型构建
首先我们要训练好模型。这里采用的是2层的LSTM框架，每层有128个隐藏层节点，batch_size设为64。训练数据来源于全唐诗（可在上面百度云资源分享当中找到）。特别注意到的一点是这里每训练完一次就对训练数据做shuffle。

### 模型训练

