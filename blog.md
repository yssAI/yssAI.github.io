
## 新年使用LSTM智能作诗送祝福

### LSTM介绍
序列化数据即每个样本和它之前的样本存在关联，前一数据和后一个数据有顺序关系。深度学习中有一个重要的分支是专门用来处理这样的数据的——循环神经网络。循环神经网络广泛应用在自然语言处理领域，今天我们带你从一个实际的例子出发，介绍神经网络一个重要的改进算法模型，LSTM。本文章不对LSTM的原理进行深入，想详细了解LSTM的可以参考下面的[文章链接](https://www.jianshu.com/p/9dc9f41f0b29)。本文重点从古诗词自动生成的实例出发，一步一步带你从数据处理到模型搭建，再到训练出古诗词生成模型，最后实现从古诗词自动生成新春祝福诗词。

### 数据处理

我们使用76748首古诗词作为数据集，数据集下载链接[]，原始的古诗词的存储形式如下：
![image](https://user-images.githubusercontent.com/43362551/51824023-221ea180-231c-11e9-8577-6595844d752f.png)
原始的古诗词是文本的形式，是一种符号的形式，无法直接进行机器学习，所以我们第一步需要把文本信息转换为数据形式，这种转换方式就叫词嵌入(word embedding)，我们采用一种常用的词嵌套(word embedding)算法-Word2vec对古诗词进行编码。关于Word2Vec这里不详细讲解，有兴趣的可以参考下面的[文章链接](https://zhuanlan.zhihu.com/p/26306795)。在词嵌套过程中，为了避免最终的分类数过于庞大，可以选择去掉出现频率较小的字，比如可以去掉只出现过一次的字。Word2vec算法经过训练后会产生一个模型文件，这样我们就可以应用这个Word2Vec模型对古诗词文本进行词嵌套编码。

经过第一步的处理已经把古诗词词语转换为可以机器学习建模的数字形式，因为我们采用循环神经网络进行古诗词生成，所以还需要构建输入到输出的映射处理。例如：
“[长河落日圆]”作为train_data，而相应的train_label就是“长河落日圆]]”，也就是
“[”->“长”，“长”->“河”，“河”->“落”，“落”->“日”，“日”->“圆”，“圆”->“]”，“]”->“]”，这样子先后顺序一一对相。这也是循环神经网络的一个重要的特征。
这里的“[”和“]”是开始符和结束符，用于生成古诗的开始与结束标记。

总结一下数据处理的步骤：
- 读取原始的古诗词文本，统计出所有不同的字，使用 Word2Vec 算法进行对应编码；
- 对于每首诗，将每个字、标点都转换为字典中对应的编号，构成神经网络的输入数据 train_data；
- 将输入数据左移动构成输出标签 train_label；

经过数据处理后我们得到以下数据文件： 
- poems_edge_split.txt：原始古诗词文件，按行排列，每行为一首诗词。  
- vectors_poem.bin：利用 Word2Vec训练好的词向量模型，以</s>开头，按词频排列，去除低频词。
- poem_ids.txt：按输入输出关系映射处理之后的语料库文件
- rhyme_words.txt： 押韵词存储，用于押韵诗的生成 


在提供的源码中已经提供了以上四个数据文件放在data文件夹下，[源码链接]()

代码如下：
``` python

```

### 模型构建
首先我们要训练好模型。这里采用的是2层的LSTM框架，每层有128个隐藏层节点，batch_size设为64。训练数据来源于全唐诗（可在上面百度云资源分享当中找到）。特别注意到的一点是这里每训练完一次就对训练数据做shuffle。

### 模型训练

